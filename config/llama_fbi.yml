base_model:
  type: llama
  path: /home/xzy/FBI-LLM/Amber #LM360/Amber
  torch_dtype: auto

quant:
  method: fbi_llm
  fbi_args:
    tag: FBI-LLM-7B
    model_size: 7B
    n_nodes: 1
    n_devices_per_node: 8
    per_device_batch_size: 4
    accumulate_grad_batches: 8
    use_kd: 1 
    config_path: /home/xzy/model-quantification-tool/quantization/fbi_llm/FBI-LLM_configs/FBI-LLM_llama2_7B.json
    learning_rate: 3e-4
    end_learning_rate: 3e-5
    warmup_grad_steps: 2000
    grad_norm_clip: 1.0
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.95
    precision: bf16-mixed
    random_seed: 11111
  data:
    name: Amber #LLM360/AmberDatasets
    nsamples: 100000 #all
    path: /home/xzy/FBI-LLM/AmberDatasets
    seed: 42

save: /home/xzy/model-quantification-tool/output/fbi_llm_test
