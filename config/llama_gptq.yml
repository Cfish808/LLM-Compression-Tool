base:
    seed: &seed 0
base_model:
    type: Llama
    path: /home/yejinyu/llama2_7b/Llama-2-7b-ms
    torch_dtype: auto
    tokenizer_mode: fast
calibrate_name: c4
skip_layers: [lm_head]
wbit: 3
w_groupsize: 128
seqlen: 2048
abit: 16
device: cuda
offload: cpu
algo: gptq
w_qtype: per_group
block_sequential: False
layer_sequential: False
eval: ceval
save: /home/yejinyu/llama2_7b/output/llama27b_miom_2