base_model:
  path: /ssd/yejinyu/EfficientQAT_1/output/block_ap_models/Llama-3-8b-w2g128_4096_mixed-precision_3mask_test_wikipedia_2.87bits
  type: llama3
  trust_remote_code: false
  use_auth_token: false
  torch_dtype: auto

quant:
  method: efficientqat_e2e
  real_quant: false
  mixed_precision: true
  maskfile_dir: /home/yejinyu/EfficientQAT_1/wikipedia_salient_columns/wikipedia_salient_columns_llama3_8b_up_lim10.json
  TrainingArguments:
    pt_context_len: 2048
    wbits: 2
    seed: 2
    bf16: true
    group_size: 128
    learning_rate: 0.00005
    weight_decay: 0.0
    max_grad_norm: 0.5
    per_device_train_batch_size: 8
    gradient_accumulation_steps: 32
    max_memory_MB: 80000
    logging_steps: 1
    log_output_dir: ./output
    save_strategy: steps

  DataArguments:
    name: redpajama
    dataset_format: pt
    conv_temp: llama-2
    cache_path: ./cache
    source_max_len: 2048
    max_train_samples: 50000
    max_eval_samples: 64


  generation_args:
    max_new_tokens: 256

eval:
  tasks: []
  datasets: [wikitext2,c4]
  seq_len: 2048
  batch_size: 4
  num_fewshot: 0
  nsamples: all
  device: cuda

save: /ssd/yejinyu/EfficientQAT_1/output/e2e-qp-output/Llama-3-8b-w2g128_5w_mixed-precision_3mask_test_wikipedia_2.87bits
