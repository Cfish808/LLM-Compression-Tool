# QLoRA Configuration File
# This file contains all the parameters used in the QLoRA training script
base_model:
    type: Llama
    model_name_or_path: /home/xzy/Llama-2-7b-hf
    torch_dtype: auto
    tokenizer_mode: fast
    use_auth_token: false  # Will use the token generated when running `transformers-cli login`
    trust_remote_code: false  # Enable unpickling of arbitrary code in AutoModelForCausalLM.from_pretrained
    output_dir: "./output"  # The output directory where the model predictions and checkpoints will be written
    cache_dir: null  # Where to store the pre-trained models downloaded from s3
    bf16: false  # Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training
    fp16: true  # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training
    gradient_checkpointing: true  # If True, use gradient checkpointing to save memory at the expense of slower backward pass
    full_finetune: false  # Whether to full finetune or do LoRA finetuning
    lora_r: 64  # Lora attention dimension
    lora_alpha: 16  # Lora alpha
    lora_dropout: 0.0  # Lora dropout
    bits: 4  # How many bits to use for quantization
    double_quant: True  # Whether to use double quantization
    quant_type: "nf4"  # Quantization data type to use. Should be one of `fp4` or `nf4`
    max_memory_MB: 80000  # Memory limit for each GPU

quant:
  method: "qlora"
  # Data Arguments
  data:
    dataset: wikitext2
    nsamples: 128
    seqlen: 2048
    download: True
    path: eval data path
    batch_size: 32
    seed: 42
    max_train_samples: 10000  # For debugging purposes or quicker training, truncate the number of training examples to this value if set.
    max_eval_samples: 1000  # For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.
    source_max_len: 1024  # Maximum length of the source input sequence
    target_max_len: 256  # Maximum length of the target output sequence
    eval_dataset_size: 1024  #
    dataset_format: null
    predict_with_generate: false
    do_train: true  # Whether to run training
    do_eval: true  # Whether to run eval on the dev set
    do_predict: true  # Whether to run predictions on the test set
    group_by_length: true  # Whether to group samples of similar length together for efficiency
    train_on_source: false  # Whether to train on the source text as well as the target text


  args:
    run_name: null  # An optional descriptor for the run
    mmlu_source_max_len: 2048  # Maximum length of the MMLU source input sequence
    max_mmlu_samples: null  # For debugging purposes or quicker training, truncate the number of MMLU evaluation examples to this value if set.
    mmlu_split: "eval"  # MMLU split to use (validation, test)
    do_mmlu_eval: false  # Whether to evaluate on MMLU dataset
    full_finetune: false  # Whether to full finetune or do LoRA finetuning
    seed: 42  # Random seed that will be set at the beginning of training
    mmlu_dataset: "mmlu-fs"  # MMLU dataset to use (mmlu-zs, mmlu, mmlu-fs)
    do_train: true  # Whether to run training
    do_eval: true  # Whether to run eval on the dev set
    do_predict: true  # Whether to run predictions on the test set
    training_args:
      per_device_train_batch_size: 32  # Batch size per GPU/TPU core/CPU for training
      gradient_accumulation_steps: 16  # Number of updates steps to accumulate before performing a backward/update pass
      learning_rate: 0.0002  # The initial learning rate for AdamW optimizer
      weight_decay: 0.0  # The weight decay to apply (if not zero)
      max_grad_norm: 0.3  # Maximum gradient norm (for gradient clipping)
      max_steps: 1000  # If > 0: set total number of training steps to perform
      lr_scheduler_type: "constant"  # The lr scheduler type to use
      warmup_ratio: 0.03  # Ratio of steps for the linear warmup (from 0 to learning_rate)
      logging_steps: 1  # Log every X updates steps
      save_strategy: "steps"  # The checkpoint save strategy to adopt during training
      save_steps: 250  # Save checkpoint every X updates steps
      save_total_limit: 3  # Limit the total amount of checkpoints
      bf16: false  # Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training
      fp16: true  # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training
      local_rank: -1  # For distributed training: local_rank
      debug: []  # Whether or not to enable debug mode
      remove_unused_columns: false  # Remove columns not required by the model when using an nlp.Dataset
      report_to: []  # Disable external reporters (e.g., wandb, tensorboard)
      gradient_checkpointing: true  # If True, use gradient checkpointing to save memory at the expense of slower backward pass
      optim: "paged_adamw_32bit"  # The optimizer to use
      output_dir: "./output"  # The output directory where the model predictions and checkpoints will be written
      # Generation Arguments
      generation_args:
        temperature: 1.0  # The value used to module the next token probabilities
        top_p: 1.0  # If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher are kept for generation
        top_k: 50  # The number of highest probability vocabulary tokens to keep for top-k-filtering
        num_beams: 1  # Number of beams for beam search
        num_beam_groups: 1
        max_new_tokens: 256  # The maximum numbers of tokens to generate
        min_new_tokens: null  # The minimum numbers of tokens to generate
        do_sample: false  # Whether or not to use sampling ; use greedy decoding otherwise
        penalty_alpha: null
        use_cache: true
        typical_p: 1.0
        diversity_penalty: 0.0
        repetition_penalty: 1.0
        length_penalty: 1.0
        no_repeat_ngram_size: 1.0

save: /home/xzy/output/llama2_7b_lora
