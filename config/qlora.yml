# QLoRA Configuration File
# This file contains all the parameters used in the QLoRA training script
base_model:
    type: Llama
    path: /home/xzy/Llama-2-7b-hf
    torch_dtype: auto
    tokenizer_mode: fast
method: "qlora"
# Model Arguments
model_args:
  model_name_or_path: "/home/xzy/Llama-2-7b-hf"  # Path to pre-trained model or model identifier from huggingface.co/models
  use_auth_token: false  # Will use the token generated when running `transformers-cli login`
  trust_remote_code: false  # Enable unpickling of arbitrary code in AutoModelForCausalLM.from_pretrained

# Data Arguments
data_args:
  dataset: "alpaca"  # The name of the dataset to use (via the datasets library)
  dataset_format: null
  max_train_samples: 10000  # For debugging purposes or quicker training, truncate the number of training examples to this value if set.
  max_eval_samples: 1000  # For debugging purposes or quicker training, truncate the number of evaluation examples to this value if set.
  source_max_len: 1024  # Maximum length of the source input sequence
  target_max_len: 256  # Maximum length of the target output sequence
  eval_dataset_size: 1024  # Size of the evaluation dataset

# Training Arguments
training_args:
  output_dir: "./output"  # The output directory where the model predictions and checkpoints will be written
  cache_dir: null  # Where to store the pre-trained models downloaded from s3
  do_train: true  # Whether to run training
  do_eval: true  # Whether to run eval on the dev set
  do_predict: true  # Whether to run predictions on the test set
  per_device_train_batch_size: 32  # Batch size per GPU/TPU core/CPU for training
  gradient_accumulation_steps: 16  # Number of updates steps to accumulate before performing a backward/update pass
  learning_rate: 0.0002  # The initial learning rate for AdamW optimizer
  weight_decay: 0.0  # The weight decay to apply (if not zero)
  max_grad_norm: 0.3  # Maximum gradient norm (for gradient clipping)
  max_steps: 1000  # If > 0: set total number of training steps to perform
  adam8bit : false
  lr_scheduler_type: "constant"  # The lr scheduler type to use
  warmup_ratio: 0.03  # Ratio of steps for the linear warmup (from 0 to learning_rate)
  logging_steps: 100  # Log every X updates steps
  save_strategy: "steps"  # The checkpoint save strategy to adopt during training
  save_steps: 250  # Save checkpoint every X updates steps
  save_total_limit: 3  # Limit the total amount of checkpoints
  seed: 42  # Random seed that will be set at the beginning of training
  bf16: false  # Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training
  fp16: true  # Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training
  local_rank: -1  # For distributed training: local_rank
  debug: []  # Whether or not to enable debug mode
  run_name: null  # An optional descriptor for the run
  remove_unused_columns: false  # Remove columns not required by the model when using an nlp.Dataset
  report_to: null  # The list of integrations to report the results and logs to
  gradient_checkpointing: true  # If True, use gradient checkpointing to save memory at the expense of slower backward pass
  full_finetune: false  # Whether to full finetune or do LoRA finetuning
  lora_r: 64  # Lora attention dimension
  lora_alpha: 16  # Lora alpha
  lora_dropout: 0.0  # Lora dropout
  bits: 4  # How many bits to use for quantization
  double_quant: True  # Whether to use double quantization
  quant_type: "nf4"  # Quantization data type to use. Should be one of `fp4` or `nf4`
  optim: "paged_adamw_32bit"  # The optimizer to use
  max_memory_MB: 80000  # Memory limit for each GPU
  group_by_length: true  # Whether to group samples of similar length together for efficiency
  train_on_source: false  # Whether to train on the source text as well as the target text
  max_mmlu_samples: null  # For debugging purposes or quicker training, truncate the number of MMLU evaluation examples to this value if set.
  mmlu_dataset: "mmlu-fs"  # MMLU dataset to use (mmlu-zs, mmlu, mmlu-fs)
  mmlu_source_max_len: 2048  # Maximum length of the MMLU source input sequence
  do_mmlu_eval: false  # Whether to evaluate on MMLU dataset
  mmlu_split: "eval"  # MMLU split to use (validation, test)
# Generation Arguments
generation_args:
  temperature: 1.0  # The value used to module the next token probabilities
  top_p: 1.0  # If set to float < 1, only the most probable tokens with probabilities that add up to `top_p` or higher are kept for generation
  top_k: 50  # The number of highest probability vocabulary tokens to keep for top-k-filtering
  num_beams: 1  # Number of beams for beam search
  num_beam_groups: 1
  max_new_tokens: 256  # The maximum numbers of tokens to generate
  min_new_tokens: null  # The minimum numbers of tokens to generate
  do_sample: false  # Whether or not to use sampling ; use greedy decoding otherwise
  penalty_alpha: null
  use_cache: true
  typical_p: 1.0
  diversity_penalty: 0.0
  repetition_penalty: 1.0
  length_penalty: 1.0
  no_repeat_ngram_size: 1.0