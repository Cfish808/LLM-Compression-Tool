base_model:
    type: Llama
    path: /home/ybh/models/Llama-2-7b-hf
    torch_dtype: auto
    tokenizer_mode: fast
quant:
    method: omniquant
    seqlen: 2048
    device: auto
    weight:
      wbit: 4
      abit: 4
      offload: cpu
      w_groupsize: 128 
      w_qtype: per_channel
      a_qtype: per_token
      alpha: 0.5
      let: True
      lwc: True
      quant_out: False
      # 训练相关参数
      epochs: 10
      let_lr: 0.005
      lwc_lr: 0.01
      wd: 0.0
      batch_size: 1
      aug_loss: False
      symmetric: False
      disable_zero_point: False
    data:
      name: wikitext2
      nsamples: 128
      seqlen: 2048
      download: True
      path: eval data path
      batch_size: 1
      seed: 42
eval:
#    eval_pos: [pretrain, fake_quant]
    tasks: [ppl]
    datasets: [wikitext2]
    download: True
    path: eval data path
    seq_len: 2048
    batch_size: 32
    num_fewshot: 0
    nsamples: all
    device: cuda
    seqlen: 2048
    # For 7B / 13B model eval, bs can be set to "1", and inference_per_block can be set to "False".
    # For 70B model eval, bs can be set to "20", and inference_per_block can be set to "True".
    bs: 1
save: /home/ybh/code/model-quantification-tool/quant_output/llama2_7b/omniquant
