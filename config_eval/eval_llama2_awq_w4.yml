# AWQ W4A16 配置
base_model:
    type: Llama
    path: /netcache/huggingface/llama2_7b/
    torch_dtype: auto
    tokenizer_mode: fast

quant:
    method: awq
    skip_layers: [ lm_head ]
    seqlen: 2048
    device: cuda
    weight:
      wbit: 4              # W4A16
      abit: 16
      offload: cpu
      block_sequential: True
      layer_sequential: True
      w_qtype: per_group
      groupsize: 128
      blocksize: 128
      w_has_zero: True
      w_unsign: True
      auto_scale: True
      auto_clip: True
      percdamp: 0.01
      actorder: False
    
    data:
      name: c4
      split: train
      nsamples: 128       
      seqlen: 2048
      download: False
      path: /netcache/huggingface/c4_local/c4-train.00000-of-01024.json.gz
      batch_size: 32
      seed: 42

eval:
    tasks: [Language_Modeling, Commonsense_Reasoning]
    datasets: [wikitext2, c4, winogrande, hellaswag, arc_easy, arc_challenge]
    download: True
    path: /netcache/huggingface/eval_data/
    seq_len: 2048
    batch_size: 16
    num_fewshot: 0
    nsamples: all
    device: cuda
    seqlen: 2048
    bs: 1
