
base_model:
    type: Llama
    path: /netcache/huggingface/llama2_7b/
    torch_dtype: auto
    tokenizer_mode: fast
quant:
    method: rtn
    skip_layers: [ lm_head ]
    seqlen: 2048
    device: cuda
    weight:
      wbit: 3
      abit: 16
      offload: cpu
      block_sequential: True
      layer_sequential: True
      w_has_zero: False
      a_has_zero: True
      w_unsign: False
      a_unsign: False
      w_groupsize: 128
      a_groupsize: -1
      a_qtype: "per_token"  #select from ["per_tensor", "per_token", "per_group", "per_dimension"]                 
      w_qtype: "per_group"  # select from ["per_tensor", "per_channel", "per_group"]             
      quantization_type: 'static'
    data:
      name: c4
      split: train
      nsamples: 128       
      seqlen: 2048
      download: False
      path: /netcache/huggingface/c4_local/c4-train.00000-of-01024.json.gz
      batch_size: 32
      seed: 42

eval:
  device: cuda
  tasks:
    - task: ppl
      datasets: [c4]   # PPL 数据集  [wikitext2, c4, ptb]
      download: False
      path: /netcache/huggingface/c4_local/c4-validation.00000-of-00008.json.gz
      seq_len: 2048
      batch_size: 32
      nsamples: all
      seqlen: 2048
save: /netcache/zcx2/compress_models/llama2_7b/llama2_7b_rtn_w3a16/