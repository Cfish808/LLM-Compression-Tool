# ============================================
# LLaMA2-7B GPTQ 量化 + 评测配置
# ============================================

base_model:
    type: Llama
    path: /netcache/huggingface/llama2_7b/
    torch_dtype: auto
    tokenizer_mode: fast

# # ============================================
# # 量化配置
# # ============================================
# quant:
#     method: gptq
#     skip_layers: [ lm_head ]
#     seqlen: 2048
#     device: cuda

#     # 权重量化参数
#     weight:
#       wbit: 3              # 量化位数: 3/4 (推荐测试这两个)
#       abit: 16             # 激活位数 (保持 FP16)
#       offload: cpu         # 权重 offload 到 CPU 节省显存
#       block_sequential: True
#       layer_sequential: True
#       w_qtype: per_group   # 分组量化
#       groupsize: 128       # 分组大小
#       blocksize: 128
#       percdamp: 0.01       # Hessian 阻尼因子
#       actorder: False      # 激活排序 (推荐开启) 代码 Bug 当前的 actorder=True 与分组量化不兼容

#     # GPTQ 特殊参数
#     special:
#       # actorder: True     # 没起作用
#       static_groups: False
#       percdamp: 0.01
#       blocksize: 128
#       true_sequential: True

#     # 校准数据
#     data:
#       name: c4
#       split: train
#       nsamples: 128       
#       seqlen: 2048
#       download: False
#       path: /netcache/huggingface/c4_local/c4-train.00000-of-01024.json.gz
#       batch_size: 32
#       seed: 42

eval:
  device: cuda
  tasks:
    - task: ppl
      datasets: [c4]   # PPL 数据集  [wikitext2, c4, ptb]
      download: False
      path: /netcache/huggingface/c4_local/c4-validation.00000-of-00008.json.gz
      seq_len: 2048
      batch_size: 32
      nsamples: all
      seqlen: 2048

# ============================================
# 保存路径 
save: /netcache/zcx2/compress_models/llama2_7b/llama2_7b_gptq_w3/
